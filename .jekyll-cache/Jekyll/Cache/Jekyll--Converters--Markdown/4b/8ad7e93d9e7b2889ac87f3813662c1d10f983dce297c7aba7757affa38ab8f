I"<h1 id="neural-networks-introduction">Neural Networks Introduction</h1>
<ul>
  <li>Computational paradigm based on biological nervous system</li>
</ul>

<p>Brain: $10^{10}$ neurons, $10^{4}$ fan in, $10^{14}$ connection strengths</p>

<h2 id="nn-learning">NN learning</h2>
<p>NN does not know anything they learnt.</p>

<ul>
  <li>Initial random paramters (weights)</li>
  <li>Small modifications to weights on each presentation of data</li>
  <li>Simple networks can be set up directly</li>
</ul>

<h2 id="nn-processing">NN processing</h2>
<ul>
  <li>Input new patterns</li>
  <li>Propagate activations along (weighted) links</li>
  <li>Repeat training</li>
</ul>

<h1 id="computational-model-of-a-single-neuron">Computational model of a single neuron</h1>

<h2 id="mcculloch--pitts">McCulloch &amp; Pitts</h2>

<p>N binary inputs x1,x2,…,xN<br />
1 binary output y<br />
threshold $\theta$<br />
N weights w1,w2,…,wN<br />
w is 1 or -1<br />
y(x) = 1 iff xi * wi &gt;= threshold for all i</p>

<p>Gating network with memory</p>

<h2 id="rosenblatt">Rosenblatt</h2>

<p>weights not fixed<br />
random interconnections<br />
learn from experience</p>

<h1 id="gradient-descent">Gradient descent</h1>

<p>Partial derivative of a function tells us how to change $w$ to minimise $f(w)$.<br />
Gradient descent updates the parameter vector $w$, using the partial derivatives of the error function to minimise the error.</p>

<h1 id="computational-model-of-perceptron">Computational model of Perceptron</h1>

<p>weights learned from data</p>
:ET